{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/erostrate9/Desktop/CSI5340 DL/Project/code/GFNEval/torchgfn\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: einops>=0.6.1 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (2.1.3)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.9.0->torchgfn==1.1.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->torchgfn==1.1.1) (3.0.2)\n",
      "Building wheels for collected packages: torchgfn\n",
      "  Building wheel for torchgfn (pyproject.toml): started\n",
      "  Building wheel for torchgfn (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for torchgfn: filename=torchgfn-1.1.1-py3-none-any.whl size=82716 sha256=cac2034c12dc936edfe94d9d8037806983eabc7b12fb455cc769b7584e03fa90\n",
      "  Stored in directory: /private/var/folders/c_/9pzrss116732p7dxch3kn_bc0000gn/T/pip-ephem-wheel-cache-7ike197q/wheels/56/de/11/edbaf478c4bdb3bf4d2dadfda48c78d0790413f2f66eee7a21\n",
      "Successfully built torchgfn\n",
      "Installing collected packages: torchgfn\n",
      "  Attempting uninstall: torchgfn\n",
      "    Found existing installation: torchgfn 1.1.1\n",
      "    Uninstalling torchgfn-1.1.1:\n",
      "      Successfully uninstalled torchgfn-1.1.1\n",
      "Successfully installed torchgfn-1.1.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../torchgfn\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "from gfn.env import DiscreteEnv\n",
    "from gfn.gflownet import GFlowNet, TBGFlowNet, SubTBGFlowNet, FMGFlowNet, DBGFlowNet\n",
    "from gfn.gym import HyperGrid2, HyperGrid\n",
    "from gfn.modules import DiscretePolicyEstimator\n",
    "from gfn.samplers import Sampler\n",
    "from gfn.utils.modules import MLP\n",
    "from gfn.states import States, DiscreteStates\n",
    "from gfn.utils.evaluation import get_random_test_set, get_sampled_test_set, evaluate_GFNEvalS, evaluate_GFNEvalS_with_monte_carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 373/1000 [00:04<00:07, 83.32it/s, loss=0.271]"
     ]
    }
   ],
   "source": [
    "# 0 - Find Available GPU resource\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# 1 - Define the environment\n",
    "# env = HyperGrid(ndim=4, height=8, R0=0.01)\n",
    "env = HyperGrid2(ndim=4, height=8, ncenters=4,\n",
    "                             seed=torch.randint(0, 10000, (1,)).item(),\n",
    "                             device_str='cpu')\n",
    "\n",
    "# 2 - Define the neural network modules\n",
    "module_PF = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions)\n",
    "module_PB = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions - 1, trunk=module_PF.trunk)\n",
    "\n",
    "# 3 - Define the estimators\n",
    "pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor)\n",
    "pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor)\n",
    "\n",
    "# 4 - Define the GFlowNet\n",
    "gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator)\n",
    "\n",
    "# 5 - Define the sampler and optimizer\n",
    "sampler = Sampler(estimator=pf_estimator)\n",
    "optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
    "\n",
    "# 6 - Train the GFlowNet\n",
    "for i in (pbar := tqdm(range(1000))):\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=16)\n",
    "    optimizer.zero_grad()\n",
    "    loss = gfn.loss(env, trajectories).to(device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 25 == 0:\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tests = 100\n",
    "test_states_sample, test_rewards_sample =  get_sampled_test_set(gfn, env, n=n_tests)\n",
    "test_states_random, test_rewards_random =  get_random_test_set(env, n=n_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set...: 100%|██████████| 100/100 [00:00<00:00, 119.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.9502670267026702. Runtime: 0.8506119251251221 seconds.\n",
      "Function 'evaluate_GFNEvalS' executed in 0.8511 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set...: 100%|██████████| 100/100 [00:00<00:00, 110.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.943971673768229. Runtime: 0.9086501598358154 seconds.\n",
      "Function 'evaluate_GFNEvalS' executed in 0.9092 seconds\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate_GFNEvalS_with_monte_carlo() got an unexpected keyword argument 'n_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m _, _, _ \u001b[38;5;241m=\u001b[39m evaluate_GFNEvalS(gfn, env, test_states_sample, test_rewards_sample)\n\u001b[1;32m      4\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m*\u001b[39m env\u001b[38;5;241m.\u001b[39mn_states\n\u001b[0;32m----> 5\u001b[0m _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_GFNEvalS_with_monte_carlo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_states_random\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_rewards_random\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m _, _, _ \u001b[38;5;241m=\u001b[39m evaluate_GFNEvalS_with_monte_carlo(gfn, env, test_states_sample, test_rewards_sample, n_samples\u001b[38;5;241m=\u001b[39mn_samples)\n",
      "File \u001b[0;32m~/miniconda3/envs/gfn/lib/python3.10/site-packages/gfn/utils/evaluation.py:18\u001b[0m, in \u001b[0;36mtimer.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     17\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start the timer\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# End the timer\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m executed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_GFNEvalS_with_monte_carlo() got an unexpected keyword argument 'n_samples'"
     ]
    }
   ],
   "source": [
    "_, _, _ = evaluate_GFNEvalS(gfn, env, test_states_random, test_rewards_random)\n",
    "_, _, _ = evaluate_GFNEvalS(gfn, env, test_states_sample, test_rewards_sample)\n",
    "\n",
    "n_samples = 20 * env.n_states\n",
    "_, _, _ = evaluate_GFNEvalS_with_monte_carlo(gfn, env, test_states_random, test_rewards_random, n_samples=n_samples)\n",
    "_, _, _ = evaluate_GFNEvalS_with_monte_carlo(gfn, env, test_states_sample, test_rewards_sample, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_setup(env : DiscreteEnv,  algo: GFlowNet):\n",
    "    gfn = None\n",
    "    sampler = None\n",
    "    optimizer = None\n",
    "\n",
    "    if algo is TBGFlowNet:\n",
    "        # The environment has a preprocessor attribute, which is used to preprocess the state before feeding it to the policy estimator\n",
    "        module_PF = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=env.n_actions\n",
    "        ).to(env.device)  # Neural network for the forward policy, with as many outputs as there are actions\n",
    "        module_PB = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=env.n_actions - 1,\n",
    "            trunk=module_PF.trunk  # We share all the parameters of P_F and P_B, except for the last layer\n",
    "        ).to(env.device)\n",
    "\n",
    "        pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor).to(env.device)\n",
    "        pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor).to(env.device)\n",
    "\n",
    "        gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator).to(env.device)\n",
    "\n",
    "        sampler = Sampler(estimator=pf_estimator)\n",
    "\n",
    "        optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "        optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
    "\n",
    "    if algo is SubTBGFlowNet:\n",
    "        # The environment has a preprocessor attribute, which is used to preprocess the state before feeding it to the policy estimator\n",
    "        module_PF = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=env.n_actions\n",
    "        ).to(env.device)  # Neural network for the forward policy, with as many outputs as there are actions\n",
    "\n",
    "        module_PB = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=env.n_actions - 1,\n",
    "            trunk=module_PF.trunk  # We share all the parameters of P_F and P_B, except for the last layer\n",
    "        ).to(env.device)\n",
    "        module_logF = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=1,  # Important for ScalarEstimators!\n",
    "        ).to(env.device)\n",
    "\n",
    "        # 3 - We define the estimators.\n",
    "        pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor).to(env.device)\n",
    "        pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor).to(env.device)\n",
    "        logF_estimator = ScalarEstimator(module=module_logF, preprocessor=env.preprocessor).to(env.device)\n",
    "\n",
    "        # 4 - We define the GFlowNet.\n",
    "        gfn = SubTBGFlowNet(pf=pf_estimator, pb=pb_estimator, logF=logF_estimator, lamda=0.9).to(env.device)\n",
    "\n",
    "        # 5 - We define the sampler and the optimizer.\n",
    "        sampler = Sampler(estimator=pf_estimator)  # We use an on-policy sampler, based on the forward policy\n",
    "\n",
    "        # Different policy parameters can have their own LR.\n",
    "        # Log F gets dedicated learning rate (typically higher).\n",
    "        optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "        optimizer.add_param_group({\"params\": gfn.logF_parameters(), \"lr\": 1e-2})\n",
    "\n",
    "    # TODO: initialize parameterizations of FMGFlowNet and DBGFlowNet\n",
    "\n",
    "    return gfn, sampler, optimizer\n",
    "\n",
    "def training(gfn: GFlowNet, sample: Sampler, optimizer, num_epochs: int = 1000) -> Sampler:\n",
    "    for i in (pbar := tqdm(range(num_epochs))):\n",
    "        trajectories = sampler.sample_trajectories(env=env, n=16)\n",
    "        optimizer.zero_grad()\n",
    "        loss = gfn.loss(env, trajectories)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 25 == 0:\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "    return sampler\n",
    "\n",
    "#TODO\n",
    "def testing(env: DiscreteEnv, gfn: GFlowNet, num_samples: int = 10000, num_epochs: int = 250):\n",
    "    test_states_sample, test_rewards_sample =  get_sampled_test_set(gfn, env, n=n_tests)\n",
    "    _, _, _ = evaluate_GFNEvalS(gfn, env, test_states_sample, test_rewards_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hyper-parameters\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "ndims =     [2, 4, 8]\n",
    "heights =   [8, 16]\n",
    "ncenters =  [2, 4, 8, 16, 32]\n",
    "# algos =     [TBGFlowNet, SubTBGFlowNet, FMGFlowNet, DBGFlowNet]\n",
    "algos =     [TBGFlowNet, SubTBGFlowNet]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
