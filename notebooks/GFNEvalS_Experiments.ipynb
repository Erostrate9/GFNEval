{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/erostrate9/Desktop/CSI5340 DL/Project/code/GFNEval/torchgfn\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: einops>=0.6.1 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (2.1.3)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.9.0->torchgfn==1.1.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->torchgfn==1.1.1) (3.0.2)\n",
      "Building wheels for collected packages: torchgfn\n",
      "  Building wheel for torchgfn (pyproject.toml): started\n",
      "  Building wheel for torchgfn (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for torchgfn: filename=torchgfn-1.1.1-py3-none-any.whl size=82819 sha256=0f5154dc9daaf72191b9a400de50b33bc2fb28062c98aa813fca48a70274e129\n",
      "  Stored in directory: /private/var/folders/c_/9pzrss116732p7dxch3kn_bc0000gn/T/pip-ephem-wheel-cache-s6ns00hc/wheels/56/de/11/edbaf478c4bdb3bf4d2dadfda48c78d0790413f2f66eee7a21\n",
      "Successfully built torchgfn\n",
      "Installing collected packages: torchgfn\n",
      "  Attempting uninstall: torchgfn\n",
      "    Found existing installation: torchgfn 1.1.1\n",
      "    Uninstalling torchgfn-1.1.1:\n",
      "      Successfully uninstalled torchgfn-1.1.1\n",
      "Successfully installed torchgfn-1.1.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../torchgfn\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "from gfn.env import DiscreteEnv\n",
    "from gfn.gflownet import GFlowNet, TBGFlowNet, SubTBGFlowNet, FMGFlowNet, DBGFlowNet\n",
    "from gfn.gym import HyperGrid2, HyperGrid\n",
    "from gfn.modules import DiscretePolicyEstimator\n",
    "from gfn.samplers import Sampler\n",
    "from gfn.utils.modules import MLP\n",
    "from gfn.states import States, DiscreteStates\n",
    "from gfn.utils.evaluation import get_random_test_set, get_sampled_test_set, evaluate_GFNEvalS, evaluate_GFNEvalS_with_monte_carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:12<00:00, 80.17it/s, loss=0.396]\n"
     ]
    }
   ],
   "source": [
    "# 0 - Find Available GPU resource\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# 1 - Define the environment\n",
    "# env = HyperGrid(ndim=4, height=8, R0=0.01)\n",
    "env = HyperGrid2(ndim=4, height=8, ncenters=4,\n",
    "                             seed=torch.randint(0, 10000, (1,)).item(),\n",
    "                             device_str='cpu')\n",
    "\n",
    "# 2 - Define the neural network modules\n",
    "module_PF = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions)\n",
    "module_PB = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions - 1, trunk=module_PF.trunk)\n",
    "\n",
    "# 3 - Define the estimators\n",
    "pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor)\n",
    "pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor)\n",
    "\n",
    "# 4 - Define the GFlowNet\n",
    "gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator)\n",
    "\n",
    "# 5 - Define the sampler and optimizer\n",
    "sampler = Sampler(estimator=pf_estimator)\n",
    "optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
    "\n",
    "# 6 - Train the GFlowNet\n",
    "for i in (pbar := tqdm(range(1000))):\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=16)\n",
    "    optimizer.zero_grad()\n",
    "    loss = gfn.loss(env, trajectories).to(device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 25 == 0:\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tests = 100\n",
    "test_states_sample, test_rewards_sample =  get_sampled_test_set(gfn, env, n=n_tests)\n",
    "test_states_random, test_rewards_random =  get_random_test_set(env, n=n_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set...: 100%|██████████| 100/100 [00:00<00:00, 118.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.90022202220222. Runtime: 0.8497319221496582 seconds.\n",
      "Function 'evaluate_GFNEvalS' executed in 0.8501 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set...: 100%|██████████| 100/100 [00:00<00:00, 152.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.9620257738166927. Runtime: 0.6603457927703857 seconds.\n",
      "Function 'evaluate_GFNEvalS' executed in 0.6609 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terminal_states: 100%|██████████| 81920/81920 [00:00<00:00, 324804.83it/s]\n",
      "Evaluating GFNEvalS with monte carlo: 100%|██████████| 100/100 [00:00<00:00, 38696.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Monte Carlo): 0.9119522762459814. MC sample number: 81920. Runtime: 10.104704856872559 seconds\n",
      "Function 'evaluate_GFNEvalS_with_monte_carlo' executed in 10.1047 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terminal_states: 100%|██████████| 81920/81920 [00:00<00:00, 314049.13it/s]\n",
      "Evaluating GFNEvalS with monte carlo: 100%|██████████| 100/100 [00:00<00:00, 34917.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Monte Carlo): 0.9741268712096439. MC sample number: 81920. Runtime: 10.017601013183594 seconds\n",
      "Function 'evaluate_GFNEvalS_with_monte_carlo' executed in 10.0176 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = evaluate_GFNEvalS(gfn, env, test_states_random, test_rewards_random)\n",
    "_, _, _ = evaluate_GFNEvalS(gfn, env, test_states_sample, test_rewards_sample)\n",
    "\n",
    "n_samples = 20 * env.n_states\n",
    "_, _, _ = evaluate_GFNEvalS_with_monte_carlo(gfn, env, test_states_random, test_rewards_random, n_samples=n_samples)\n",
    "_, _, _ = evaluate_GFNEvalS_with_monte_carlo(gfn, env, test_states_sample, test_rewards_sample, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_setup(env : DiscreteEnv,  algo: GFlowNet):\n",
    "    gfn = None\n",
    "    sampler = None\n",
    "    optimizer = None\n",
    "\n",
    "    if algo is TBGFlowNet:\n",
    "        # The environment has a preprocessor attribute, which is used to preprocess the state before feeding it to the policy estimator\n",
    "        module_PF = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=env.n_actions\n",
    "        ).to(env.device)  # Neural network for the forward policy, with as many outputs as there are actions\n",
    "        module_PB = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=env.n_actions - 1,\n",
    "            trunk=module_PF.trunk  # We share all the parameters of P_F and P_B, except for the last layer\n",
    "        ).to(env.device)\n",
    "\n",
    "        pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor).to(env.device)\n",
    "        pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor).to(env.device)\n",
    "\n",
    "        gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator).to(env.device)\n",
    "\n",
    "        sampler = Sampler(estimator=pf_estimator)\n",
    "\n",
    "        optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "        optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
    "\n",
    "    if algo is SubTBGFlowNet:\n",
    "        # The environment has a preprocessor attribute, which is used to preprocess the state before feeding it to the policy estimator\n",
    "        module_PF = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=env.n_actions\n",
    "        ).to(env.device)  # Neural network for the forward policy, with as many outputs as there are actions\n",
    "\n",
    "        module_PB = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=env.n_actions - 1,\n",
    "            trunk=module_PF.trunk  # We share all the parameters of P_F and P_B, except for the last layer\n",
    "        ).to(env.device)\n",
    "        module_logF = MLP(\n",
    "            input_dim=env.preprocessor.output_dim,\n",
    "            output_dim=1,  # Important for ScalarEstimators!\n",
    "        ).to(env.device)\n",
    "\n",
    "        # 3 - We define the estimators.\n",
    "        pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor).to(env.device)\n",
    "        pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor).to(env.device)\n",
    "        logF_estimator = ScalarEstimator(module=module_logF, preprocessor=env.preprocessor).to(env.device)\n",
    "\n",
    "        # 4 - We define the GFlowNet.\n",
    "        gfn = SubTBGFlowNet(pf=pf_estimator, pb=pb_estimator, logF=logF_estimator, lamda=0.9).to(env.device)\n",
    "\n",
    "        # 5 - We define the sampler and the optimizer.\n",
    "        sampler = Sampler(estimator=pf_estimator)  # We use an on-policy sampler, based on the forward policy\n",
    "\n",
    "        # Different policy parameters can have their own LR.\n",
    "        # Log F gets dedicated learning rate (typically higher).\n",
    "        optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "        optimizer.add_param_group({\"params\": gfn.logF_parameters(), \"lr\": 1e-2})\n",
    "\n",
    "    # TODO: initialize parameterizations of FMGFlowNet and DBGFlowNet\n",
    "\n",
    "    return gfn, sampler, optimizer\n",
    "\n",
    "def training(gfn: GFlowNet, sample: Sampler, optimizer, num_epochs: int = 1000) -> Sampler:\n",
    "    for i in (pbar := tqdm(range(num_epochs))):\n",
    "        trajectories = sampler.sample_trajectories(env=env, n=16)\n",
    "        optimizer.zero_grad()\n",
    "        loss = gfn.loss(env, trajectories)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 25 == 0:\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "    return sampler\n",
    "\n",
    "#TODO\n",
    "def testing(env: DiscreteEnv, gfn: GFlowNet, num_samples: int = 10000, num_epochs: int = 250):\n",
    "    test_states_sample, test_rewards_sample =  get_sampled_test_set(gfn, env, n=n_tests)\n",
    "    _, _, _ = evaluate_GFNEvalS(gfn, env, test_states_sample, test_rewards_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hyper-parameters\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "ndims =     [2, 4, 8]\n",
    "heights =   [8, 16]\n",
    "ncenters =  [2, 4, 8, 16, 32]\n",
    "# algos =     [TBGFlowNet, SubTBGFlowNet, FMGFlowNet, DBGFlowNet]\n",
    "algos =     [TBGFlowNet, SubTBGFlowNet]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
