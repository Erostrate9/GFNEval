{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "-cXehRxJbHxb",
        "outputId": "eac13e8f-3e55-427d-a6a7-071b0e7a4982",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://sdawzy:****@github.com/Erostrate9/GFNEval.git#subdirectory=torchgfn\n",
            "  Cloning https://sdawzy:****@github.com/Erostrate9/GFNEval.git to /tmp/pip-req-build-ptl67x5b\n",
            "  Running command git clone --filter=blob:none --quiet 'https://sdawzy:****@github.com/Erostrate9/GFNEval.git' /tmp/pip-req-build-ptl67x5b\n",
            "  Resolved https://sdawzy:****@github.com/Erostrate9/GFNEval.git to commit cb6fcd4370790301808f630059ecd501bb4fe459\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torchgfn\n",
            "  Building wheel for torchgfn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchgfn: filename=torchgfn-1.1.1-py3-none-any.whl size=79472 sha256=96bf1cc91638920397d2d0313c766b807e810b690563c897e1f4d15bc7007340\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ch25jz76/wheels/97/74/20/5c5130c3639d55c9ed0b3f7f003fa1a07cb97f41d8198db048\n",
            "Successfully built torchgfn\n",
            "Installing collected packages: torchgfn\n",
            "Successfully installed torchgfn-1.1.1\n"
          ]
        }
      ],
      "source": [
        "#@title Install Dependencies\n",
        "!find . -name \"*.pyc\" -delete\n",
        "!find . -name \"__pycache__\" -delete\n",
        "# !pip install --no-deps git+https://github.com/GFNOrg/torchgfn.git\n",
        "# Replace with your GitHub username and personal access token\n",
        "username = \"sdawzy\"\n",
        "token = \"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\"\n",
        "\n",
        "# Replace with your private repository URL\n",
        "repo_url = \"https://github.com/Erostrate9/GFNEval.git\"\n",
        "\n",
        "!pip install --no-deps git+https://{username}:{token}@{repo_url.split('https://')[1]}#subdirectory=torchgfn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Necessary Packages\n",
        "import torch\n",
        "torch.set_default_dtype(torch.float)\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from gfn.gflownet import GFlowNet, TBGFlowNet, SubTBGFlowNet, FMGFlowNet, DBGFlowNet\n",
        "from gfn.samplers import Sampler\n",
        "from gfn.env import Env\n",
        "from gfn.modules import DiscretePolicyEstimator, ScalarEstimator\n",
        "from gfn.utils.modules import MLP  # is a simple multi-layer perceptron (MLP)\n",
        "from gfn.containers import Trajectories\n",
        "from gfn.states import States\n",
        "\n",
        "from gfn.eval_kl import PhiFunction, calc_KL_using_model, compute_KL\n",
        "from gfn.gym.hypergrid2 import HyperGrid2, get_final_states\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cSWpNpmXbSx-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Experiment Setup, Traing, and Testing\n",
        "def experiment_setup(env : Env,  algo: GFlowNet):\n",
        "    gfn = None\n",
        "    sampler = None\n",
        "    optimizer = None\n",
        "\n",
        "    if algo is TBGFlowNet:\n",
        "        # The environment has a preprocessor attribute, which is used to preprocess the state before feeding it to the policy estimator\n",
        "        module_PF = MLP(\n",
        "            input_dim=env.preprocessor.output_dim,\n",
        "            output_dim=env.n_actions\n",
        "        ).to(env.device)  # Neural network for the forward policy, with as many outputs as there are actions\n",
        "        module_PB = MLP(\n",
        "            input_dim=env.preprocessor.output_dim,\n",
        "            output_dim=env.n_actions - 1,\n",
        "            trunk=module_PF.trunk  # We share all the parameters of P_F and P_B, except for the last layer\n",
        "        ).to(env.device)\n",
        "\n",
        "        pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor).to(env.device)\n",
        "        pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor).to(env.device)\n",
        "\n",
        "        gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator).to(env.device)\n",
        "\n",
        "        sampler = Sampler(estimator=pf_estimator)\n",
        "\n",
        "        optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
        "        optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
        "\n",
        "    if algo is SubTBGFlowNet:\n",
        "        # The environment has a preprocessor attribute, which is used to preprocess the state before feeding it to the policy estimator\n",
        "        module_PF = MLP(\n",
        "            input_dim=env.preprocessor.output_dim,\n",
        "            output_dim=env.n_actions\n",
        "        ).to(env.device)  # Neural network for the forward policy, with as many outputs as there are actions\n",
        "\n",
        "        module_PB = MLP(\n",
        "            input_dim=env.preprocessor.output_dim,\n",
        "            output_dim=env.n_actions - 1,\n",
        "            trunk=module_PF.trunk  # We share all the parameters of P_F and P_B, except for the last layer\n",
        "        ).to(env.device)\n",
        "        module_logF = MLP(\n",
        "            input_dim=env.preprocessor.output_dim,\n",
        "            output_dim=1,  # Important for ScalarEstimators!\n",
        "        ).to(env.device)\n",
        "\n",
        "        # 3 - We define the estimators.\n",
        "        pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor).to(env.device)\n",
        "        pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor).to(env.device)\n",
        "        logF_estimator = ScalarEstimator(module=module_logF, preprocessor=env.preprocessor).to(env.device)\n",
        "\n",
        "        # 4 - We define the GFlowNet.\n",
        "        gfn = SubTBGFlowNet(pf=pf_estimator, pb=pb_estimator, logF=logF_estimator, lamda=0.9).to(env.device)\n",
        "\n",
        "        # 5 - We define the sampler and the optimizer.\n",
        "        sampler = Sampler(estimator=pf_estimator)  # We use an on-policy sampler, based on the forward policy\n",
        "\n",
        "        # Different policy parameters can have their own LR.\n",
        "        # Log F gets dedicated learning rate (typically higher).\n",
        "        optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
        "        optimizer.add_param_group({\"params\": gfn.logF_parameters(), \"lr\": 1e-2})\n",
        "\n",
        "    # TODO: initialize parameterizations of FMGFlowNet and DBGFlowNet\n",
        "\n",
        "    return gfn, sampler, optimizer\n",
        "\n",
        "def training(gfn: GFlowNet, sample: Sampler, optimizer, num_epochs: int = 1000) -> Sampler:\n",
        "    for i in (pbar := tqdm(range(num_epochs))):\n",
        "        trajectories = sampler.sample_trajectories(env=env, n=16)\n",
        "        optimizer.zero_grad()\n",
        "        loss = gfn.loss(env, trajectories)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 25 == 0:\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "    return sampler\n",
        "\n",
        "def testing(env: Env, gfn: GFlowNet, num_samples: int = 10000, num_epochs: int = 250, show_progress: bool = False) -> None:\n",
        "    # Sample from proxy distribution\n",
        "    # i.e. from the learned sampler\n",
        "    samples_proxy_distribution = gfn.sample_terminating_states(env=env, n=num_samples)\n",
        "    samples_proxy_tensor = samples_proxy_distribution.tensor.double().to(env.device)\n",
        "\n",
        "    # Sample from the true distribution\n",
        "    samples_true_distribution = env.sample_states_from_distribution(num_samples)\n",
        "    samples_true_tensor = samples_true_distribution.tensor.double().to(env.device)\n",
        "\n",
        "    kl, phi = compute_KL(samples_proxy_tensor, samples_true_tensor,\n",
        "                         num_epochs=num_epochs, show_progress=show_progress)\n",
        "    return kl, phi\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Z_0YZjmDgjDe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyper-parameters\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "ndims =     [2, 4, 8, 16]\n",
        "heights =   [8, 16, 32, 64, 128, 256]\n",
        "ncenters =  [2, 4, 8, 16, 32]\n",
        "algos =     [TBGFlowNet, SubTBGFlowNet, FMGFlowNet, DBGFlowNet]"
      ],
      "metadata": {
        "id": "mEmrHka3c2XJ",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Progress\n",
        "drive.mount('/content/drive')\n",
        "drive_path = \"/content/drive/My Drive/KLEval_TB_results.pkl\"\n",
        "\n",
        "\n",
        "# Load saved results\n",
        "try:\n",
        "    with open(drive_path, \"rb\") as f:\n",
        "        results = pickle.load(f)\n",
        "    print(\"Loaded saved results from Google Drive.\")\n",
        "except FileNotFoundError:\n",
        "    results = {}  # Start fresh if no file exists"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "-lG2Ns5QYsSI",
        "outputId": "f361af3a-ea0b-4857-ec6c-c771c1d191fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded saved results from Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some test\n",
        "env = HyperGrid2(ndim=4, height=256, ncenters=4,\n",
        "                             seed=torch.randint(0, 10000, (1,)).item(),\n",
        "                             device_str='cpu')\n",
        "sampler = results[(4,256,4)]['sampler']\n",
        "sampler.sample_trajectories(env=env, n=10)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "pa_RmzSymxjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Optional) Reset results\n",
        "results = {}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vd6RK7GO3vun"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start experiments from here:"
      ],
      "metadata": {
        "id": "0c7tu8ATejYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Specify the algorithm\n",
        "algo = TBGFlowNet\n",
        "for ndim in ndims:\n",
        "    for height in heights:\n",
        "        if height ** ndim > 1e8:\n",
        "            continue\n",
        "        for ncenter in ncenters:\n",
        "            if (ndim, height, ncenter) in results and 'seed' in results[(ndim, height, ncenter)]:\n",
        "                print(f\"Skipping already processed: ndim={ndim}, height={height}, ncenter={ncenter}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"ndim={ndim}, height={height}, ncenter={ncenter}, algo=TB\")\n",
        "            seed = torch.randint(0, 10000, (1,)).item()\n",
        "            env = HyperGrid2(ndim=ndim, height=height, ncenters=ncenter,\n",
        "                             seed=seed,\n",
        "                             device_str=device_str)\n",
        "            gfn, sampler, optimizer = experiment_setup(env, algo)\n",
        "\n",
        "            sampler = training(gfn, sampler, optimizer)\n",
        "            # Save partial results\n",
        "            results[(ndim, height, ncenter)] = {\n",
        "                'sampler': sampler,  # Save the sampler object\n",
        "                'gfn': gfn,\n",
        "                'optimizer': optimizer,\n",
        "                'seed': seed,\n",
        "                'device_str': device_str,\n",
        "                # 'env': env,\n",
        "                'ndim': ndim,\n",
        "                'height': height,\n",
        "                'ncenter': ncenter,\n",
        "            }\n",
        "            with open(drive_path, \"wb\") as f:\n",
        "                pickle.dump(results, f)\n",
        "            print(f\"Partial results saved to {drive_path}\")\n",
        "\n",
        "            # Calculate KL and phi\n",
        "            kl, phi = testing(env, gfn)\n",
        "            print(f\"KL={kl}\")\n",
        "\n",
        "            # Save results\n",
        "            results[(ndim, height, ncenter)].update({\n",
        "                'kl': kl,\n",
        "                'phi': phi,\n",
        "            })\n",
        "            with open(drive_path, \"wb\") as f:\n",
        "                pickle.dump(results, f)\n",
        "            print(f\"Results saved to {drive_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0j0Zz99ecBO",
        "outputId": "d307a80a-9883-4dfd-cbe4-584e6341d674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ndim=2, height=8, ncenter=2, algo=TB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:20<00:00, 12.50it/s, loss=0.000562]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partial results saved to /content/drive/My Drive/KLEval_TB_results.pkl\n",
            "KL=0.08946420041107372\n",
            "Results saved to /content/drive/My Drive/KLEval_TB_results.pkl\n",
            "ndim=2, height=8, ncenter=4, algo=TB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 773/1000 [00:48<00:15, 14.67it/s, loss=0.0023]"
          ]
        }
      ]
    }
  ]
}