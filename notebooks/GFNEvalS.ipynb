{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ptVYB2WbHFx"
   },
   "source": [
    "# Install dependency in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wi6Q7515ciib"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../torchgfn\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewF0o8QWbJHe"
   },
   "source": [
    "# GFNEvalS Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BaPTfCCslZQi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gfn.gflownet import TBGFlowNet\n",
    "from gfn.gym import HyperGrid\n",
    "from gfn.modules import DiscretePolicyEstimator\n",
    "from gfn.samplers import Sampler\n",
    "from gfn.utils.modules import MLP\n",
    "from gfn.states import States, DiscreteStates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GFlowNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUZCEfP2k6_N",
    "outputId": "b550fc78-b1bf-4785-e451-28b7adaadd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 67.16it/s, loss=0.215]\n"
     ]
    }
   ],
   "source": [
    "# 0 - Find Available GPU resource\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1 - Define the environment\n",
    "env = HyperGrid(ndim=4, height=8, R0=0.01)\n",
    "\n",
    "# 2 - Define the neural network modules\n",
    "module_PF = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions)\n",
    "module_PB = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions - 1, trunk=module_PF.trunk)\n",
    "\n",
    "# 3 - Define the estimators\n",
    "pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor)\n",
    "pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor)\n",
    "\n",
    "# 4 - Define the GFlowNet\n",
    "gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator)\n",
    "\n",
    "# 5 - Define the sampler and optimizer\n",
    "sampler = Sampler(estimator=pf_estimator)\n",
    "optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
    "\n",
    "# 6 - Train the GFlowNet\n",
    "for i in (pbar := tqdm(range(1000))):\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=16)\n",
    "    optimizer.zero_grad()\n",
    "    loss = gfn.loss(env, trajectories).to(device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 25 == 0:\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get all possible states from the environment\n",
    "# height**ndim\n",
    "env.all_states\n",
    "assert len(env.all_states)==env.height**env.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_actions = ndim + 1\n",
    "# actions are represented by a number in {0, …, n_actions - 1}, the last one being the exit action.\n",
    "env.n_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sampling Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'((6, 6), (2, 0))': True, '((1, 2), (3, 4))': False, '(0, 0, 0, 1)': True}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "class TensorDict:\n",
    "    def __init__(self, default_factory=None):\n",
    "        # Use a defaultdict with an optional default factory\n",
    "        self.data = defaultdict(default_factory)\n",
    "        self.default_factory = default_factory\n",
    "\n",
    "    def _tensor_to_hashable(self, tensor):\n",
    "        # Recursively convert a tensor to a hashable structure (nested tuples)\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            return self._tensor_to_hashable(tensor.tolist())\n",
    "        elif isinstance(tensor, list):\n",
    "            return tuple(self._tensor_to_hashable(item) for item in tensor)\n",
    "        else:\n",
    "            return tensor  # Base case: numbers are already hashable\n",
    "\n",
    "    def __setitem__(self, tensor, value):\n",
    "        # Convert tensor to a hashable structure for storage\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        self.data[key] = value\n",
    "\n",
    "    def __getitem__(self, tensor):\n",
    "        # Retrieve value based on hashable structure\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        return self.data[key]\n",
    "\n",
    "    def __contains__(self, tensor):\n",
    "        # Check existence based on hashable structure\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        return key in self.data\n",
    "\n",
    "    def __str__(self):\n",
    "        # Pretty-print the dictionary content as tuples and values\n",
    "        pretty_dict = {\n",
    "            str(key): value for key, value in self.data.items()\n",
    "        }\n",
    "        return str(pretty_dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Provide a developer-friendly representation\n",
    "        return f\"TensorDict({self.__str__()})\"\n",
    "\n",
    "# Example usage\n",
    "dic = TensorDict(default_factory=lambda: False)\n",
    "\n",
    "x1 = torch.tensor([[6, 6], [2, 0]])\n",
    "x2 = torch.tensor([[1, 2], [3, 4]])\n",
    "x3 = torch.tensor([0, 0, 0, 1])\n",
    "\n",
    "dic[x1] = True\n",
    "dic[x2] = False\n",
    "dic[x3] = True\n",
    "\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_transition_log_probs(env, pf_estimator):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "    Returns transition_log_probs, a Tensor list with length of env.n_actions. \n",
    "        transition_log_probs[i][j] indicates the log probability of taking action i at a State env.all_states[j], i in [0, n_actions-1]\n",
    "    \"\"\"\n",
    "    all_states = env.all_states\n",
    "    estimator_output = pf_estimator(all_states)\n",
    "    dist = pf_estimator.to_probability_distribution(all_states, estimator_output)\n",
    "    transition_log_probs = [None] * env.n_actions\n",
    "    \n",
    "    for i in tqdm(range(env.n_actions)):\n",
    "        action = torch.Tensor([i])\n",
    "        transition_log_probs[i] = dist.log_prob(action)\n",
    "    return transition_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log \\pi_\\theta(s) = \\log \\left( \\sum_{s{\\prime} \\in \\text{Parent}(s)} \\exp \\left( \\log P_{F_\\theta}(s | s{\\prime}) + \\log \\pi_\\theta(s{\\prime}) \\right) \\right)$$\n",
    "\n",
    "where $ P_{F_\\theta}(s | s{\\prime})  $ is the forward transition probability, and s is a state in the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SbSucqUOyNLi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from gfn.states import stack_states\n",
    "# TODO\n",
    "def compute_log_probability(env: HyperGrid, gfn, state: DiscreteStates, memo, transition_log_probs):\n",
    "    \"\"\"\n",
    "    Recursively computes the log of the sampling probability π_θ(s) for a given terminal state `state`\n",
    "    in a GFlowNet `gfn` using torchgfn library.\n",
    "\n",
    "    Args:\n",
    "        gfn (GFlowNet): The GFlowNet model instance.\n",
    "        state (States): The terminal state for which we want to compute log π_θ(s).\n",
    "        memo (dict): A dictionary for memoization to store previously computed log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The log probability π_θ(s).\n",
    "    \"\"\"\n",
    "    # Check if the result is already computed and stored in memo\n",
    "    if state.tensor in memo:\n",
    "        return memo[state.tensor]\n",
    "\n",
    "    # Base case: if the state is the initial state, log π_θ(s_initial) = 0\n",
    "    if state.is_initial_state.all():\n",
    "        log_prob = torch.tensor([0.0], requires_grad=False)\n",
    "        memo[state.tensor] = log_prob\n",
    "        return log_prob\n",
    "    \n",
    "    # Recursive case: compute log π_θ(s) from all parent states\n",
    "    # Collect log-probabilities for each parent transition\n",
    "    log_probs = []\n",
    "    # to iterate each parent state and the corresponding action\n",
    "    for i in range(env.n_actions-1):\n",
    "        action = env.actions_from_tensor(torch.Tensor([[i]]).to(torch.int64))\n",
    "        env.update_masks(state)\n",
    "        if env.is_action_valid(state, action, backward=True):\n",
    "            # s'\n",
    "            parent_state_tensor = env.backward_step(state, action)\n",
    "            parent_state = env.states_from_tensor(parent_state_tensor)\n",
    "            # parent_state = stack_states([parent_state])\n",
    "            parent_state_idx = env.get_states_indices(parent_state)\n",
    "            # logPF(s|s'): Forward transition probability in log form\n",
    "            log_forward_prob = transition_log_probs[i][parent_state_idx]\n",
    "            # log π_θ(s'): Recursively compute log π_θ(parent_state)\n",
    "            log_parent_prob=compute_log_probability(env, gfn, parent_state, memo, transition_log_probs)\n",
    "            # Compute the sum inside the exponent for this parent\n",
    "            log_probs.append(log_forward_prob + log_parent_prob)\n",
    "    # Sum of exponentiated log-probabilities (log-sum-exp trick for numerical stability)\n",
    "    log_prob = torch.logsumexp(torch.stack(log_probs), dim=0)\n",
    "    # Memoize and return\n",
    "    memo[state.tensor] = log_prob\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 3912.60it/s]\n"
     ]
    }
   ],
   "source": [
    "transition_log_probs = get_all_transition_log_probs(env, pf_estimator=pf_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.9369], grad_fn=<LogsumexpBackward0>)\n",
      "{'((0, 0, 0, 0),)': tensor([0.]), '((0, 0, 0, 1),)': tensor([-1.4087], grad_fn=<LogsumexpBackward0>), '((0, 0, 0, 2),)': tensor([-1.9369], grad_fn=<LogsumexpBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "memo = TensorDict(default_factory=lambda: torch.tensor([-float('inf')], requires_grad=False))\n",
    "state = stack_states([env.all_states[2]])\n",
    "log_prob=compute_log_probability(env, gfn, state, memo, transition_log_probs)\n",
    "print(log_prob)\n",
    "print(memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ifU5HtMPmuKS"
   },
   "outputs": [],
   "source": [
    "# 8 - Generate a test set and compute probabilities\n",
    "n_test = 100  # Number of test trajectories\n",
    "test_trajectories = sampler.sample_trajectories(env=env, n=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 3936.83it/s]\n"
     ]
    }
   ],
   "source": [
    "transition_log_probs = get_all_transition_log_probs(env, pf_estimator=pf_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute GFNEvalS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob_termination(env: HyperGrid, terminal_state: DiscreteStates, memo, transition_log_probs):\n",
    "    terminal_state_tensor = terminal_state.tensor\n",
    "    termination_action = env.actions_from_tensor(torch.Tensor([[env.n_actions-1]]).to(torch.int64))\n",
    "    env.update_masks(terminal_state)\n",
    "    assert env.is_action_valid(terminal_state, termination_action, backward=False), f\"Error: Termination at given state {terminal_state.tensor} is invalid!\"\n",
    "    terminal_state_idx = env.get_states_indices(terminal_state)\n",
    "    # log π_θ(s_terminal) + log termination\n",
    "    return memo[terminal_state_tensor] + transition_log_probs[-1][terminal_state_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_terminal: tensor([[0, 7, 6, 6]])\n",
      "log π_θ(s_terminal): tensor([-5.5142], grad_fn=<LogsumexpBackward0>)\n",
      "log_prob when termination at s_terminal: tensor([-6.4989], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "terminal_state = test_trajectories[0].states[-2]\n",
    "terminal_state_tensor = terminal_state.tensor\n",
    "print(f's_terminal: {terminal_state_tensor}')\n",
    "print(f'log π_θ(s_terminal): {memo[terminal_state_tensor]}')\n",
    "log_prob_termination = compute_log_prob_termination(env, terminal_state, memo, transition_log_probs)\n",
    "print(f'log_prob when termination at s_terminal: {log_prob_termination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trajectories: 100%|██████████| 100/100 [00:00<00:00, 118.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.870835310870831. Runtime: 0.8839941024780273 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize lists to hold the probabilities and rewards\n",
    "# transition_log_probs = get_all_transition_log_probs(env, pf_estimator=pf_estimator)\n",
    "log_probs = []\n",
    "log_probs_termination = []\n",
    "log_rewards = []\n",
    "memo = memo = TensorDict(default_factory=lambda: torch.tensor(['-inf'], requires_grad=False))\n",
    "# Calculate the log probability and log reward for each terminal state\n",
    "# for traj in test_trajectories:\n",
    "for traj in tqdm(test_trajectories, desc=\"Processing trajectories\"):\n",
    "    terminal_state = traj.states[-2]\n",
    "    reward = env.reward(terminal_state)\n",
    "    log_reward = torch.log(reward)\n",
    "    log_prob=compute_log_probability(env, gfn, terminal_state, memo, transition_log_probs)\n",
    "    log_prob_termination = compute_log_prob_termination(env, terminal_state, memo, transition_log_probs) \n",
    "    log_probs.append(log_prob.detach().numpy())\n",
    "    log_probs_termination.append(log_prob_termination.detach().numpy())\n",
    "    log_rewards.append(log_reward.detach().numpy())\n",
    "\n",
    "# 9 - Compute Spearman's Rank Correlation\n",
    "spearman_corr_termination, _ = spearmanr(log_probs_termination, log_rewards)\n",
    "print(f\"Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): {spearman_corr_termination}. Runtime: {time.time()-start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Original GFNEvalS, excluding termination actions): 0.6554558780175926\n"
     ]
    }
   ],
   "source": [
    "# 9 - Compute Spearman's Rank Correlation (Original GFNEvalS, excluding termination actions)\n",
    "spearman_corr, _ = spearmanr(log_probs, log_rewards)\n",
    "print(f\"Spearman's Rank Correlation (Original GFNEvalS, excluding termination actions): {spearman_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.870835310870831\n"
     ]
    }
   ],
   "source": [
    "# 10 - Compute Spearman's Rank Correlation (Modified GFNEvalS, including termination actions)\n",
    "spearman_corr_termination, _ = spearmanr(log_probs_termination, log_rewards)\n",
    "print(f\"Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): {spearman_corr_termination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## an approximation of sampling probability with monte carlo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gfn.samplers import Sampler\n",
    "from collections import Counter\n",
    "from gfn.states import States\n",
    "\n",
    "# to compute the sampling probability wit monte_carlo\n",
    "def count_occurrences_with_monte_carlo(env, sampler, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Computes the sampling probability of a given terminal state using Monte Carlo.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance.\n",
    "        sampler: An initialized Sampler using the forward policy estimator.\n",
    "        terminal_state: The terminal state whose probability we want to compute (as a tensor).\n",
    "        n_samples: The number of trajectories to sample.\n",
    "\n",
    "    Returns:\n",
    "        occurrences: occurrences dict of each state.tensor\n",
    "    \"\"\"\n",
    "    # Sample trajectories\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=n_samples)\n",
    "    # Extract terminal states\n",
    "    terminal_states = [traj.states[-2] for traj in trajectories]\n",
    "    occurrences = TensorDict(int)\n",
    "    for state in tqdm(terminal_states, desc=\"Processing terminal_states\"):\n",
    "        occurrences[state.tensor]+=1 \n",
    "    return occurrences\n",
    "def compute_log_prob_with_monte_carlo(occurrences, terminal_state, n_samples: int) -> float:\n",
    "    # Calculate the probability\n",
    "    if isinstance(terminal_state, States):\n",
    "        terminal_state = terminal_state.tensor\n",
    "    return torch.log(torch.tensor(occurrences[terminal_state] / n_samples, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terminal_states: 100%|██████████| 81920/81920 [00:00<00:00, 317874.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log_prob of the terminal state via Monte Carlo [[6, 1, 1, 6]]: -4.52565336227417\n",
      "Log_prob of the terminal state via GFNEvalS [[6, 1, 1, 6]]: tensor([-4.1811], grad_fn=<LogsumexpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trajectories: 100%|██████████| 100/100 [00:00<00:00, 10651.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Monte Carlo): 0.8709764566238493. MC sample number: 81920. Runtime: 7.86014986038208 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Define the terminal state (replace with the actual state representation)\n",
    "terminal_state = torch.tensor([[6, 1, 1, 6]])\n",
    "# Compute the sampling probability\n",
    "n_samples = 20 * env.n_states\n",
    "occurrences = count_occurrences_with_monte_carlo(env, sampler, n_samples=n_samples)\n",
    "\n",
    "# \n",
    "terminal_state = torch.tensor([[6, 1, 1, 6]])\n",
    "log_prob = compute_log_prob_with_monte_carlo(occurrences, terminal_state, n_samples)\n",
    "print(f\"Log_prob of the terminal state via Monte Carlo {terminal_state.tolist()}: {log_prob}\")\n",
    "print(f\"Log_prob of the terminal state via GFNEvalS {terminal_state.tolist()}: {memo[terminal_state]}\")\n",
    "# \n",
    "log_probs_monte_carlo = []\n",
    "log_rewards_monte_carlo = []\n",
    "for traj in tqdm(test_trajectories, desc=\"Processing trajectories\"):\n",
    "    terminal_state = traj.states[-2]\n",
    "    reward = env.reward(terminal_state)\n",
    "    log_reward = torch.log(reward)\n",
    "    log_prob=compute_log_prob_with_monte_carlo(occurrences, terminal_state, n_samples)\n",
    "    log_probs_monte_carlo.append(log_prob.detach().numpy())\n",
    "    log_rewards_monte_carlo.append(log_reward.detach().numpy())\n",
    "# Compute Spearman's Rank Correlation\n",
    "spearman_corr_monte_carlo, _ = spearmanr(log_probs_monte_carlo, log_rewards_monte_carlo)\n",
    "print(f\"Spearman's Rank Correlation (Monte Carlo): {spearman_corr_monte_carlo}. MC sample number: {n_samples}. Runtime: {time.time()-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
