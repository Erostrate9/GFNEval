{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ptVYB2WbHFx"
   },
   "source": [
    "# Install dependency in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wi6Q7515ciib"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../torchgfn\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewF0o8QWbJHe"
   },
   "source": [
    "# Demo (Pseudocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BaPTfCCslZQi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gfn.gflownet import TBGFlowNet\n",
    "from gfn.gym import HyperGrid\n",
    "from gfn.modules import DiscretePolicyEstimator\n",
    "from gfn.samplers import Sampler\n",
    "from gfn.utils.modules import MLP\n",
    "from gfn.states import DiscreteStates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUZCEfP2k6_N",
    "outputId": "b550fc78-b1bf-4785-e451-28b7adaadd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:13<00:00, 72.53it/s, loss=0.169]\n"
     ]
    }
   ],
   "source": [
    "# 0 - Find Available GPU resource\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1 - Define the environment\n",
    "env = HyperGrid(ndim=4, height=8, R0=0.01)\n",
    "\n",
    "# 2 - Define the neural network modules\n",
    "module_PF = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions)\n",
    "module_PB = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions - 1, trunk=module_PF.trunk)\n",
    "\n",
    "# 3 - Define the estimators\n",
    "pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor)\n",
    "pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor)\n",
    "\n",
    "# 4 - Define the GFlowNet\n",
    "gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator)\n",
    "\n",
    "# 5 - Define the sampler and optimizer\n",
    "sampler = Sampler(estimator=pf_estimator)\n",
    "optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
    "\n",
    "# 6 - Train the GFlowNet\n",
    "for i in (pbar := tqdm(range(1000))):\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=16)\n",
    "    optimizer.zero_grad()\n",
    "    loss = gfn.loss(env, trajectories).to(device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 25 == 0:\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "class TensorDict:\n",
    "    def __init__(self, default_factory=None):\n",
    "        # Use a defaultdict with an optional default factory\n",
    "        self.data = defaultdict(default_factory)\n",
    "        self.default_factory = default_factory\n",
    "\n",
    "    def _tensor_to_hashable(self, tensor):\n",
    "        # Recursively convert a tensor to a hashable structure (nested tuples)\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            return self._tensor_to_hashable(tensor.tolist())\n",
    "        elif isinstance(tensor, list):\n",
    "            return tuple(self._tensor_to_hashable(item) for item in tensor)\n",
    "        else:\n",
    "            return tensor  # Base case: numbers are already hashable\n",
    "\n",
    "    def __setitem__(self, tensor, value):\n",
    "        # Convert tensor to a hashable structure for storage\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        self.data[key] = value\n",
    "\n",
    "    def __getitem__(self, tensor):\n",
    "        # Retrieve value based on hashable structure\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        return self.data[key]\n",
    "\n",
    "    def __contains__(self, tensor):\n",
    "        # Check existence based on hashable structure\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        return key in self.data\n",
    "\n",
    "# Example usage\n",
    "dic = TensorDict(default_factory=lambda: False)\n",
    "x1 = torch.tensor([[0, 0, 6, 1]])\n",
    "x2 = torch.tensor([[0, 0, 6, 1]])\n",
    "x3 = torch.tensor([[0, 0, 6, 2]])\n",
    "x4 = torch.tensor([0, 0, 6, 1])\n",
    "dic[x1] = True\n",
    "assert (x2 in dic) == True\n",
    "assert (x3 not in dic) == True\n",
    "assert (x4 not in dic) == True\n",
    "print(dic[x2])  # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbSucqUOyNLi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# TODO\n",
    "def compute_log_probability(gfn, state, memo):\n",
    "    \"\"\"\n",
    "    Recursively computes the log of the sampling probability π_θ(s) for a given terminal state `state`\n",
    "    in a GFlowNet `gfn` using torchgfn library.\n",
    "\n",
    "    Args:\n",
    "        gfn (GFlowNet): The GFlowNet model instance.\n",
    "        state (States): The terminal state for which we want to compute log π_θ(s).\n",
    "        memo (dict): A dictionary for memoization to store previously computed log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The log probability π_θ(s).\n",
    "    \"\"\"\n",
    "    # Check if the result is already computed and stored in memo\n",
    "    if state in memo:\n",
    "        return memo[state]\n",
    "\n",
    "    # Base case: if the state is the initial state, log π_θ(s_initial) = 0\n",
    "    if state.is_initial_state.all():\n",
    "        log_prob = torch.tensor(0.0, requires_grad=False)\n",
    "        memo[state] = log_prob\n",
    "        return log_prob\n",
    "\n",
    "    # Recursive case: compute log π_θ(s) from parent states\n",
    "    # TODO: how to get the parents states?\n",
    "    parent_states = get_parents(state)\n",
    "\n",
    "    # Collect log-probabilities for each parent transition\n",
    "    log_probs = []\n",
    "    for parent_state in parent_states:\n",
    "        # Forward transition probability in log form\n",
    "        log_forward_prob = torch.log(gfn.get_forward_transition_probability(state, parent_state))\n",
    "\n",
    "        # Recursively compute log π_θ(parent_state)\n",
    "        log_parent_prob = compute_log_probability(gfn, parent_state, memo)\n",
    "\n",
    "        # Compute the sum inside the exponent for this parent\n",
    "        log_probs.append(log_forward_prob + log_parent_prob)\n",
    "\n",
    "    # Sum of exponentiated log-probabilities (log-sum-exp trick for numerical stability)\n",
    "    log_prob = torch.logsumexp(torch.stack(log_probs), dim=0)\n",
    "\n",
    "    # Memoize and return\n",
    "    memo[state] = log_prob\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ifU5HtMPmuKS"
   },
   "outputs": [],
   "source": [
    "# 8 - Generate a test set and compute probabilities\n",
    "n_test = 100  # Number of test trajectories\n",
    "test_trajectories = sampler.sample_trajectories(env=env, n=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectories(n_trajectories=1, max_length=8, First 10 trajectories:states=\n",
       "[0 0 0 0]-> [0 0 1 0]-> [0 0 2 0]-> [0 0 3 0]-> [0 0 4 0]-> [0 0 5 0]-> [0 0 6 0]-> [0 0 6 1]-> [-1 -1 -1 -1]\n",
       "when_is_done=[8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each trajactories instance contains n_trajectories trajactories, probably with different lengths\n",
    "# in this case, it only contains one trajactory, with a length of 15.\n",
    "test_trajectories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0]])\n",
      "states[0] is_initial_state: tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# states[0] denotes the initial state\n",
    "print(test_trajectories[0].states[0].tensor)\n",
    "print(f'states[0] is_initial_state: {test_trajectories[0].states[0].is_initial_state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1, -1, -1, -1]])\n",
      "states[-1] is_sink_state: tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# states[-1] denotes the sink state\n",
    "print(test_trajectories[0].states[-1].tensor)\n",
    "print(f'states[-1] is_sink_state: {test_trajectories[0].states[-1].is_sink_state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 6, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trajectories[0].states[-2].tensor.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log \\pi_\\theta(s) = \\log \\left( \\sum_{s{\\prime} \\in \\text{Parent}(s)} \\exp \\left( \\log P_{F_\\theta}(s | s{\\prime}) + \\log \\pi_\\theta(s{\\prime}) \\right) \\right)$$\n",
    "\n",
    "where $ P_{F_\\theta}(s | s{\\prime})  $ is the forward transition probability, and s is a state in the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TensorDict object at 0x40c81dc30>\n",
      "Sampling probability of the terminal state [6, 1, 1, 6]: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gfn.samplers import Sampler\n",
    "from collections import Counter\n",
    "from gfn.states import States\n",
    "\n",
    "# to compute the sampling probability wit monte_carlo\n",
    "def count_sampling_probability_with_monte_carlo(env, sampler, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Computes the sampling probability of a given terminal state using Monte Carlo.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance.\n",
    "        sampler: An initialized Sampler using the forward policy estimator.\n",
    "        terminal_state: The terminal state whose probability we want to compute (as a tensor).\n",
    "        n_samples: The number of trajectories to sample.\n",
    "\n",
    "    Returns:\n",
    "        occurrences: occurrences dict of each state.tensor\n",
    "    \"\"\"\n",
    "    # Sample trajectories\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=n_samples)\n",
    "    # Extract terminal states\n",
    "    terminal_states = [traj.states[-2] for traj in trajectories]\n",
    "    occurrences = TensorDict(int)\n",
    "    for state in terminal_states:\n",
    "        occurrences[state.tensor]+=1 \n",
    "    return occurrences\n",
    "def compute_sampling_probability_with_monte_carlo(occurrences, terminal_state, n_samples: int) -> float:\n",
    "    # Calculate the probability\n",
    "    if  isinstance(terminal_state, States):\n",
    "        terminal_state = terminal_state.tensor\n",
    "    return occurrences[terminal_state] / n_samples\n",
    "\n",
    "# Define the terminal state (replace with the actual state representation)\n",
    "terminal_state = torch.tensor([[6, 1, 1, 6]])\n",
    "# Compute the sampling probability\n",
    "n_samples = 100000\n",
    "occurrences = count_sampling_probability_with_monte_carlo(env, sampler, n_samples=n_samples)\n",
    "print(occurrences)\n",
    "sampling_probability = compute_sampling_probability_with_monte_carlo(occurrences, terminal_state, n_samples)\n",
    "print(f\"Sampling probability of the terminal state {terminal_state.tolist()}: {sampling_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00922"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_state = torch.tensor([[6, 1, 1, 6]])\n",
    "compute_sampling_probability_with_monte_carlo(occurrences, terminal_state, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgI8NYD7lcqD"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to hold the probabilities and rewards\n",
    "log_probs = []\n",
    "log_rewards = []\n",
    "memo = TensorDict()\n",
    "# Calculate the log probability and log reward for each terminal state\n",
    "for traj in test_trajectories:\n",
    "    terminal_states = traj[-1].states\n",
    "    reward = env.reward(terminal_state)\n",
    "    log_reward = np.log(reward)\n",
    "    # TODO\n",
    "    log_prob = compute_log_probability(gfn, terminal_states, memo)\n",
    "    log_probs.append(log_prob)\n",
    "    log_rewards.append(log_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_C2XlrnOmzKA"
   },
   "outputs": [],
   "source": [
    "# 9 - Compute Spearman's Rank Correlation\n",
    "spearman_corr, _ = spearmanr(log_probs, log_rewards)\n",
    "print(f\"Spearman's Rank Correlation (GFNEvalS): {spearman_corr}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
