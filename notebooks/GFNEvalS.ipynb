{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ptVYB2WbHFx"
   },
   "source": [
    "# Install dependency in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wi6Q7515ciib"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../torchgfn\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewF0o8QWbJHe"
   },
   "source": [
    "# Demo (Pseudocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BaPTfCCslZQi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gfn.gflownet import TBGFlowNet\n",
    "from gfn.gym import HyperGrid\n",
    "from gfn.modules import DiscretePolicyEstimator\n",
    "from gfn.samplers import Sampler\n",
    "from gfn.utils.modules import MLP\n",
    "from gfn.states import DiscreteStates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUZCEfP2k6_N",
    "outputId": "b550fc78-b1bf-4785-e451-28b7adaadd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 68.39it/s, loss=0.129]\n"
     ]
    }
   ],
   "source": [
    "# 0 - Find Available GPU resource\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1 - Define the environment\n",
    "env = HyperGrid(ndim=4, height=8, R0=0.01)\n",
    "\n",
    "# 2 - Define the neural network modules\n",
    "module_PF = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions)\n",
    "module_PB = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions - 1, trunk=module_PF.trunk)\n",
    "\n",
    "# 3 - Define the estimators\n",
    "pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor)\n",
    "pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor)\n",
    "\n",
    "# 4 - Define the GFlowNet\n",
    "gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator)\n",
    "\n",
    "# 5 - Define the sampler and optimizer\n",
    "sampler = Sampler(estimator=pf_estimator)\n",
    "optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
    "\n",
    "# 6 - Train the GFlowNet\n",
    "for i in (pbar := tqdm(range(1000))):\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=16)\n",
    "    optimizer.zero_grad()\n",
    "    loss = gfn.loss(env, trajectories).to(device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 25 == 0:\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "SbSucqUOyNLi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# TODO\n",
    "def compute_log_probability(gfn, state, memo={}):\n",
    "    \"\"\"\n",
    "    Recursively computes the log of the sampling probability π_θ(s) for a given terminal state `state`\n",
    "    in a GFlowNet `gfn` using torchgfn library.\n",
    "\n",
    "    Args:\n",
    "        gfn (GFlowNet): The GFlowNet model instance.\n",
    "        state (States): The terminal state for which we want to compute log π_θ(s).\n",
    "        memo (dict): A dictionary for memoization to store previously computed log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The log probability π_θ(s).\n",
    "    \"\"\"\n",
    "    # Check if the result is already computed and stored in memo\n",
    "    if state in memo:\n",
    "        return memo[state]\n",
    "\n",
    "    # Base case: if the state is the initial state, log π_θ(s_initial) = 0\n",
    "    if state.is_initial_state.all():\n",
    "        log_prob = torch.tensor(0.0, requires_grad=False)\n",
    "        memo[state] = log_prob\n",
    "        return log_prob\n",
    "\n",
    "    # Recursive case: compute log π_θ(s) from parent states\n",
    "    # TODO: how to get the parents states?\n",
    "    parent_states = get_parents(state)\n",
    "\n",
    "    # Collect log-probabilities for each parent transition\n",
    "    log_probs = []\n",
    "    for parent_state in parent_states:\n",
    "        # Forward transition probability in log form\n",
    "        log_forward_prob = torch.log(gfn.get_forward_transition_probability(state, parent_state))\n",
    "\n",
    "        # Recursively compute log π_θ(parent_state)\n",
    "        log_parent_prob = compute_log_probability(gfn, parent_state, memo)\n",
    "\n",
    "        # Compute the sum inside the exponent for this parent\n",
    "        log_probs.append(log_forward_prob + log_parent_prob)\n",
    "\n",
    "    # Sum of exponentiated log-probabilities (log-sum-exp trick for numerical stability)\n",
    "    log_prob = torch.logsumexp(torch.stack(log_probs), dim=0)\n",
    "\n",
    "    # Memoize and return\n",
    "    memo[state] = log_prob\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ifU5HtMPmuKS"
   },
   "outputs": [],
   "source": [
    "# 8 - Generate a test set and compute probabilities\n",
    "n_test = 100  # Number of test trajectories\n",
    "test_trajectories = sampler.sample_trajectories(env=env, n=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectories(n_trajectories=1, max_length=15, First 10 trajectories:states=\n",
       "[0 0 0 0]-> [0 0 1 0]-> [0 1 1 0]-> [0 2 1 0]-> [1 2 1 0]-> [2 2 1 0]-> [2 3 1 0]-> [3 3 1 0]-> [4 3 1 0]-> [4 4 1 0]-> [5 4 1 0]-> [5 5 1 0]-> [5 6 1 0]-> [6 6 1 0]-> [6 6 2 0]-> [-1 -1 -1 -1]\n",
       "when_is_done=[15])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each trajactories instance contains n_trajectories trajactories, probably with different lengths\n",
    "# in this case, it only contains one trajactory, with a length of 15.\n",
    "test_trajectories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0]])\n",
      "states[0] is_initial_state: tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# states[0] denotes the initial state\n",
    "print(test_trajectories[0].states[0].tensor)\n",
    "print(f'states[0] is_initial_state: {test_trajectories[0].states[0].is_initial_state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1, -1, -1, -1]])\n",
      "states[-1] is_sink_state: tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# states[-1] denotes the sink state\n",
    "print(test_trajectories[0].states[-1].tensor)\n",
    "print(f'states[-1] is_sink_state: {test_trajectories[0].states[-1].is_sink_state}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log \\pi_\\theta(s) = \\log \\left( \\sum_{s{\\prime} \\in \\text{Parent}(s)} \\exp \\left( \\log P_{F_\\theta}(s | s{\\prime}) + \\log \\pi_\\theta(s{\\prime}) \\right) \\right)$$\n",
    "\n",
    "where $ P_{F_\\theta}(s | s{\\prime})  $ is the forward transition probability, and s is a state in the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling probability of the terminal state [6, 6, 2, 0]: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gfn.samplers import Sampler\n",
    "from collections import Counter\n",
    "\n",
    "# Define the function to compute the sampling probability\n",
    "def compute_sampling_probability_with_monte_carlo(env, sampler, terminal_state, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Computes the sampling probability of a given terminal state using Monte Carlo.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance.\n",
    "        sampler: An initialized Sampler using the forward policy estimator.\n",
    "        terminal_state: The terminal state whose probability we want to compute (as a tensor).\n",
    "        n_samples: The number of trajectories to sample.\n",
    "\n",
    "    Returns:\n",
    "        float: The estimated sampling probability of the terminal state.\n",
    "    \"\"\"\n",
    "    # Sample trajectories\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=n_samples)\n",
    "\n",
    "    # Extract terminal states\n",
    "    terminal_states = trajectories[0].states[-2]\n",
    "\n",
    "    # Convert terminal states to a hashable form (tuple) for counting\n",
    "    terminal_states_tuples = [tuple(state.tolist()) for state in terminal_states.tensor]\n",
    "\n",
    "    # Count occurrences of the terminal state\n",
    "    terminal_state_tuple = tuple(terminal_state.tolist())\n",
    "    occurrences = Counter(terminal_states_tuples)\n",
    "\n",
    "    # Calculate the probability\n",
    "    sampling_probability = occurrences[terminal_state_tuple] / n_samples\n",
    "    return sampling_probability\n",
    "\n",
    "\n",
    "# Define the terminal state (replace with the actual state representation)\n",
    "terminal_state = torch.tensor([6, 6, 2, 0])\n",
    "\n",
    "# Compute the sampling probability\n",
    "sampling_probability = compute_sampling_probability_with_monte_carlo(env, sampler, terminal_state, n_samples=10000)\n",
    "print(f\"Sampling probability of the terminal state {terminal_state.tolist()}: {sampling_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgI8NYD7lcqD"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to hold the probabilities and rewards\n",
    "log_probs = []\n",
    "log_rewards = []\n",
    "memo = {}\n",
    "# Calculate the log probability and log reward for each terminal state\n",
    "for traj in test_trajectories:\n",
    "    terminal_states = traj[-1].states\n",
    "    reward = env.reward(terminal_state)\n",
    "    log_reward = np.log(reward)\n",
    "    # TODO\n",
    "    log_prob = compute_log_probability(gfn, terminal_states, memo)\n",
    "    log_probs.append(log_prob)\n",
    "    log_rewards.append(log_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_C2XlrnOmzKA"
   },
   "outputs": [],
   "source": [
    "# 9 - Compute Spearman's Rank Correlation\n",
    "spearman_corr, _ = spearmanr(log_probs, log_rewards)\n",
    "print(f\"Spearman's Rank Correlation (GFNEvalS): {spearman_corr}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
