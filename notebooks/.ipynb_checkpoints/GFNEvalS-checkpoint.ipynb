{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ptVYB2WbHFx"
   },
   "source": [
    "# Install dependency in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "Wi6Q7515ciib"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/erostrate9/Desktop/CSI5340 DL/Project/code/GFNEval/torchgfn\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: einops>=0.6.1 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (2.1.3)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torchgfn==1.1.1) (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from torch>=1.9.0->torchgfn==1.1.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.9.0->torchgfn==1.1.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->torchgfn==1.1.1) (3.0.2)\n",
      "Building wheels for collected packages: torchgfn\n",
      "  Building wheel for torchgfn (pyproject.toml): started\n",
      "  Building wheel for torchgfn (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for torchgfn: filename=torchgfn-1.1.1-py3-none-any.whl size=82659 sha256=0aaa942db23e923d172421a5e00a64411452a8d8bcbe2a4071cd59d367f371d4\n",
      "  Stored in directory: /private/var/folders/c_/9pzrss116732p7dxch3kn_bc0000gn/T/pip-ephem-wheel-cache-8gu9mafz/wheels/56/de/11/edbaf478c4bdb3bf4d2dadfda48c78d0790413f2f66eee7a21\n",
      "Successfully built torchgfn\n",
      "Installing collected packages: torchgfn\n",
      "  Attempting uninstall: torchgfn\n",
      "    Found existing installation: torchgfn 1.1.1\n",
      "    Uninstalling torchgfn-1.1.1:\n",
      "      Successfully uninstalled torchgfn-1.1.1\n",
      "Successfully installed torchgfn-1.1.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../torchgfn\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewF0o8QWbJHe"
   },
   "source": [
    "# GFNEvalS Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaPTfCCslZQi"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HyperGrid2' from 'gfn.gym' (/Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages/gfn/gym/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[328], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgfn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgflownet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GFlowNet, TBGFlowNet, SubTBGFlowNet, FMGFlowNet, DBGFlowNet\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgfn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HyperGrid, HyperGrid2\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgfn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiscretePolicyEstimator\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgfn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sampler\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HyperGrid2' from 'gfn.gym' (/Users/erostrate9/miniconda3/envs/gfn/lib/python3.10/site-packages/gfn/gym/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gfn.gflownet import GFlowNet\n",
    "from gfn.gym import HyperGrid, HyperGrid2\n",
    "from gfn.modules import DiscretePolicyEstimator\n",
    "from gfn.samplers import Sampler\n",
    "from gfn.utils.modules import MLP\n",
    "from gfn.states import States, DiscreteStates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GFlowNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUZCEfP2k6_N",
    "outputId": "b550fc78-b1bf-4785-e451-28b7adaadd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 67.16it/s, loss=0.215]\n"
     ]
    }
   ],
   "source": [
    "# 0 - Find Available GPU resource\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1 - Define the environment\n",
    "env = HyperGrid(ndim=4, height=8, R0=0.01)\n",
    "\n",
    "# 2 - Define the neural network modules\n",
    "module_PF = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions)\n",
    "module_PB = MLP(input_dim=env.preprocessor.output_dim, output_dim=env.n_actions - 1, trunk=module_PF.trunk)\n",
    "\n",
    "# 3 - Define the estimators\n",
    "pf_estimator = DiscretePolicyEstimator(module_PF, env.n_actions, is_backward=False, preprocessor=env.preprocessor)\n",
    "pb_estimator = DiscretePolicyEstimator(module_PB, env.n_actions, is_backward=True, preprocessor=env.preprocessor)\n",
    "\n",
    "# 4 - Define the GFlowNet\n",
    "gfn = TBGFlowNet(logZ=0., pf=pf_estimator, pb=pb_estimator)\n",
    "\n",
    "# 5 - Define the sampler and optimizer\n",
    "sampler = Sampler(estimator=pf_estimator)\n",
    "optimizer = torch.optim.Adam(gfn.pf_pb_parameters(), lr=1e-3)\n",
    "optimizer.add_param_group({\"params\": gfn.logz_parameters(), \"lr\": 1e-1})\n",
    "\n",
    "# 6 - Train the GFlowNet\n",
    "for i in (pbar := tqdm(range(1000))):\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=16)\n",
    "    optimizer.zero_grad()\n",
    "    loss = gfn.loss(env, trajectories).to(device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 25 == 0:\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get all possible states from the environment\n",
    "# height**ndim\n",
    "env.all_states\n",
    "assert len(env.all_states)==env.height**env.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_actions = ndim + 1\n",
    "# actions are represented by a number in {0, …, n_actions - 1}, the last one being the exit action.\n",
    "env.n_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sampling Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'((6, 6), (2, 0))': True, '((1, 2), (3, 4))': False, '(0, 0, 0, 1)': True}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "class TensorDict:\n",
    "    def __init__(self, default_factory=None):\n",
    "        # Use a defaultdict with an optional default factory\n",
    "        self.data = defaultdict(default_factory)\n",
    "        self.default_factory = default_factory\n",
    "\n",
    "    def _tensor_to_hashable(self, tensor):\n",
    "        # Recursively convert a tensor to a hashable structure (nested tuples)\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            return self._tensor_to_hashable(tensor.tolist())\n",
    "        elif isinstance(tensor, list):\n",
    "            return tuple(self._tensor_to_hashable(item) for item in tensor)\n",
    "        else:\n",
    "            return tensor  # Base case: numbers are already hashable\n",
    "\n",
    "    def __setitem__(self, tensor, value):\n",
    "        # Convert tensor to a hashable structure for storage\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        self.data[key] = value\n",
    "\n",
    "    def __getitem__(self, tensor):\n",
    "        # Retrieve value based on hashable structure\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        return self.data[key]\n",
    "\n",
    "    def __contains__(self, tensor):\n",
    "        # Check existence based on hashable structure\n",
    "        key = self._tensor_to_hashable(tensor)\n",
    "        return key in self.data\n",
    "\n",
    "    def __str__(self):\n",
    "        # Pretty-print the dictionary content as tuples and values\n",
    "        pretty_dict = {\n",
    "            str(key): value for key, value in self.data.items()\n",
    "        }\n",
    "        return str(pretty_dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Provide a developer-friendly representation\n",
    "        return f\"TensorDict({self.__str__()})\"\n",
    "\n",
    "# Example usage\n",
    "dic = TensorDict(default_factory=lambda: False)\n",
    "\n",
    "x1 = torch.tensor([[6, 6], [2, 0]])\n",
    "x2 = torch.tensor([[1, 2], [3, 4]])\n",
    "x3 = torch.tensor([0, 0, 0, 1])\n",
    "\n",
    "dic[x1] = True\n",
    "dic[x2] = False\n",
    "dic[x3] = True\n",
    "\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_transition_log_probs(env, pf_estimator):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "    Returns transition_log_probs, a Tensor list with length of env.n_actions. \n",
    "        transition_log_probs[i][j] indicates the log probability of taking action i at a State env.all_states[j], i in [0, n_actions-1]\n",
    "    \"\"\"\n",
    "    all_states = env.all_states\n",
    "    estimator_output = pf_estimator(all_states)\n",
    "    dist = pf_estimator.to_probability_distribution(all_states, estimator_output)\n",
    "    transition_log_probs = [None] * env.n_actions\n",
    "    \n",
    "    for i in range(env.n_actions):\n",
    "        action = torch.Tensor([i])\n",
    "        transition_log_probs[i] = dist.log_prob(action)\n",
    "    return transition_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log \\pi_\\theta(s) = \\log \\left( \\sum_{s{\\prime} \\in \\text{Parent}(s)} \\exp \\left( \\log P_{F_\\theta}(s | s{\\prime}) + \\log \\pi_\\theta(s{\\prime}) \\right) \\right)$$\n",
    "\n",
    "where $ P_{F_\\theta}(s | s{\\prime})  $ is the forward transition probability, and s is a state in the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbSucqUOyNLi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from gfn.states import stack_states\n",
    "def compute_log_probability(env: HyperGrid, gfn, state: DiscreteStates, memo, transition_log_probs):\n",
    "    \"\"\"\n",
    "    Recursively computes the log of the sampling probability π_θ(s) for a given terminal state `state`\n",
    "    in a GFlowNet `gfn` using torchgfn library.\n",
    "\n",
    "    Args:\n",
    "        gfn (GFlowNet): The GFlowNet model instance.\n",
    "        state (States): The terminal state for which we want to compute log π_θ(s).\n",
    "        memo (dict): A dictionary for memoization to store previously computed log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The log probability π_θ(s).\n",
    "    \"\"\"\n",
    "    if len(state.tensor.shape)==1:\n",
    "        state = stack_states([state])\n",
    "    # Check if the result is already computed and stored in memo\n",
    "    if state.tensor in memo:\n",
    "        return memo[state.tensor]\n",
    "\n",
    "    # Base case: if the state is the initial state, log π_θ(s_initial) = 0\n",
    "    if state.is_initial_state.all():\n",
    "        log_prob = torch.tensor([0.0], requires_grad=False)\n",
    "        memo[state.tensor] = log_prob\n",
    "        return log_prob\n",
    "    \n",
    "    # Recursive case: compute log π_θ(s) from all parent states\n",
    "    # Collect log-probabilities for each parent transition\n",
    "    log_probs = []\n",
    "    # to iterate each parent state and the corresponding action\n",
    "    for i in range(env.n_actions-1):\n",
    "        action = env.actions_from_tensor(torch.Tensor([[i]]).to(torch.int64))\n",
    "        env.update_masks(state)\n",
    "        if env.is_action_valid(state, action, backward=True):\n",
    "            # s'\n",
    "            parent_state_tensor = env.backward_step(state, action)\n",
    "            parent_state = env.states_from_tensor(parent_state_tensor)\n",
    "            # parent_state = stack_states([parent_state])\n",
    "            parent_state_idx = env.get_states_indices(parent_state)\n",
    "            # logPF(s|s'): Forward transition probability in log form\n",
    "            log_forward_prob = transition_log_probs[i][parent_state_idx]\n",
    "            # log π_θ(s'): Recursively compute log π_θ(parent_state)\n",
    "            log_parent_prob=compute_log_probability(env, gfn, parent_state, memo, transition_log_probs)\n",
    "            # Compute the sum inside the exponent for this parent\n",
    "            log_probs.append(log_forward_prob + log_parent_prob)\n",
    "    # Sum of exponentiated log-probabilities (log-sum-exp trick for numerical stability)\n",
    "    log_prob = torch.logsumexp(torch.stack(log_probs), dim=0)\n",
    "    # Memoize and return\n",
    "    memo[state.tensor] = log_prob\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "id": "ifU5HtMPmuKS"
   },
   "outputs": [],
   "source": [
    "# 8 - Generate a test set and compute probabilities\n",
    "n_test = 100  # Number of test trajectories\n",
    "test_trajectories = sampler.sample_trajectories(env=env, n=n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute GFNEvalS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob_termination(env: HyperGrid, terminal_state: DiscreteStates, memo, transition_log_probs):\n",
    "    if len(terminal_state.tensor.shape)==1:\n",
    "        terminal_state = stack_states([terminal_state])\n",
    "    terminal_state_tensor = terminal_state.tensor\n",
    "    termination_action = env.actions_from_tensor(torch.Tensor([[env.n_actions-1]]).to(torch.int64))\n",
    "    env.update_masks(terminal_state)\n",
    "    assert env.is_action_valid(terminal_state, termination_action, backward=False), f\"Error: Termination at given state {terminal_state.tensor} is invalid!\"\n",
    "    terminal_state_idx = env.get_states_indices(terminal_state)\n",
    "    # log π_θ(s_terminal) + log termination\n",
    "    return memo[terminal_state_tensor] + transition_log_probs[-1][terminal_state_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trajectories: 100%|██████████| 100/100 [00:00<00:00, 102.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.8283671542265011. Runtime: 0.9845030307769775 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize lists to hold the probabilities and rewards\n",
    "# transition_log_probs = get_all_transition_log_probs(env, pf_estimator=pf_estimator)\n",
    "log_probs = []\n",
    "log_probs_termination = []\n",
    "log_rewards = []\n",
    "memo = TensorDict(default_factory=lambda: torch.tensor(['-inf'], requires_grad=False))\n",
    "# Calculate the log probability and log reward for each terminal state\n",
    "# for traj in test_trajectories:\n",
    "for traj in tqdm(test_trajectories, desc=\"Processing trajectories\"):\n",
    "    terminal_state = traj.states[-2]\n",
    "    reward = env.reward(terminal_state)\n",
    "    log_reward = torch.log(reward)\n",
    "    log_prob=compute_log_probability(env, gfn, terminal_state, memo, transition_log_probs)\n",
    "    log_prob_termination = compute_log_prob_termination(env, terminal_state, memo, transition_log_probs) \n",
    "    log_probs.append(log_prob.detach().numpy())\n",
    "    log_probs_termination.append(log_prob_termination.detach().numpy())\n",
    "    log_rewards.append(log_reward.detach().numpy())\n",
    "\n",
    "# 9 - Compute Spearman's Rank Correlation\n",
    "spearman_corr_termination, _ = spearmanr(log_probs_termination, log_rewards)\n",
    "print(f\"Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): {spearman_corr_termination}. Runtime: {time.time()-start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_terminal: tensor([[6, 1, 6, 0]])\n",
      "log π_θ(s_terminal): tensor([-4.3418], grad_fn=<LogsumexpBackward0>)\n",
      "log_prob when termination at s_terminal: tensor([-5.9106], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "terminal_state = test_trajectories[0].states[-2]\n",
    "terminal_state_tensor = terminal_state.tensor\n",
    "print(f's_terminal: {terminal_state_tensor}')\n",
    "print(f'log π_θ(s_terminal): {memo[terminal_state_tensor]}')\n",
    "log_prob_termination = compute_log_prob_termination(env, terminal_state, memo, transition_log_probs)\n",
    "print(f'log_prob when termination at s_terminal: {log_prob_termination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Original GFNEvalS, excluding termination actions): 0.567878302378827\n"
     ]
    }
   ],
   "source": [
    "# 9 - Compute Spearman's Rank Correlation (Original GFNEvalS, excluding termination actions)\n",
    "spearman_corr, _ = spearmanr(log_probs, log_rewards)\n",
    "print(f\"Spearman's Rank Correlation (Original GFNEvalS, excluding termination actions): {spearman_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.8283671542265011\n"
     ]
    }
   ],
   "source": [
    "# 10 - Compute Spearman's Rank Correlation (Modified GFNEvalS, including termination actions)\n",
    "spearman_corr_termination, _ = spearmanr(log_probs_termination, log_rewards)\n",
    "print(f\"Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): {spearman_corr_termination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## an approximation of sampling probability with monte carlo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gfn.samplers import Sampler\n",
    "from collections import Counter\n",
    "from gfn.states import States\n",
    "\n",
    "# to compute the sampling probability wit monte_carlo\n",
    "def count_occurrences_with_monte_carlo(env, sampler, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Computes the sampling probability of a given terminal state using Monte Carlo.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance.\n",
    "        sampler: An initialized Sampler using the forward policy estimator.\n",
    "        terminal_state: The terminal state whose probability we want to compute (as a tensor).\n",
    "        n_samples: The number of trajectories to sample.\n",
    "\n",
    "    Returns:\n",
    "        occurrences: occurrences dict of each state.tensor\n",
    "    \"\"\"\n",
    "    # Sample trajectories\n",
    "    trajectories = sampler.sample_trajectories(env=env, n=n_samples)\n",
    "    # Extract terminal states\n",
    "    terminal_states = [traj.states[-2] for traj in trajectories]\n",
    "    occurrences = TensorDict(int)\n",
    "    for state in tqdm(terminal_states, desc=\"Processing terminal_states\"):\n",
    "        occurrences[state.tensor]+=1 \n",
    "    return occurrences\n",
    "def compute_log_prob_with_monte_carlo(occurrences, terminal_state, n_samples: int) -> float:\n",
    "    # Calculate the probability\n",
    "    if isinstance(terminal_state, States):\n",
    "        if len(terminal_state.tensor.shape)==1:\n",
    "            terminal_state = stack_states([terminal_state])\n",
    "        terminal_state = terminal_state.tensor\n",
    "    return torch.log(torch.tensor(occurrences[terminal_state] / n_samples, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terminal_states: 100%|██████████| 81920/81920 [00:00<00:00, 316073.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log_prob of the terminal state via Monte Carlo [[6, 1, 1, 6]]: -4.631389617919922\n",
      "Log_prob of the terminal state via GFNEvalS [[6, 1, 1, 6]]: tensor([-4.1811], grad_fn=<LogsumexpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trajectories: 100%|██████████| 100/100 [00:00<00:00, 11005.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Monte Carlo): 0.8284914593953175. MC sample number: 81920. Runtime: 7.849664926528931 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Define the terminal state (replace with the actual state representation)\n",
    "terminal_state = torch.tensor([[6, 1, 1, 6]])\n",
    "# Compute the sampling probability\n",
    "n_samples = 20 * env.n_states\n",
    "occurrences = count_occurrences_with_monte_carlo(env, sampler, n_samples=n_samples)\n",
    "\n",
    "# \n",
    "terminal_state = torch.tensor([[6, 1, 1, 6]])\n",
    "log_prob = compute_log_prob_with_monte_carlo(occurrences, terminal_state, n_samples)\n",
    "print(f\"Log_prob of the terminal state via Monte Carlo {terminal_state.tolist()}: {log_prob}\")\n",
    "print(f\"Log_prob of the terminal state via GFNEvalS {terminal_state.tolist()}: {memo[terminal_state]}\")\n",
    "# \n",
    "log_probs_monte_carlo = []\n",
    "log_rewards_monte_carlo = []\n",
    "for traj in tqdm(test_trajectories, desc=\"Processing trajectories\"):\n",
    "    terminal_state = traj.states[-2]\n",
    "    reward = env.reward(terminal_state)\n",
    "    log_reward = torch.log(reward)\n",
    "    log_prob=compute_log_prob_with_monte_carlo(occurrences, terminal_state, n_samples)\n",
    "    log_probs_monte_carlo.append(log_prob.detach().numpy())\n",
    "    log_rewards_monte_carlo.append(log_reward.detach().numpy())\n",
    "# Compute Spearman's Rank Correlation\n",
    "spearman_corr_monte_carlo, _ = spearmanr(log_probs_monte_carlo, log_rewards_monte_carlo)\n",
    "print(f\"Spearman's Rank Correlation (Monte Carlo): {spearman_corr_monte_carlo}. MC sample number: {n_samples}. Runtime: {time.time()-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Start the timer\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()  # End the timer\n",
    "        print(f\"Function '{func.__name__}' executed in {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_test_set(env: HyperGrid, n=100):\n",
    "    random_indices = torch.randperm(len(env.all_states))[:n]\n",
    "    terminal_states = env.all_states[random_indices]\n",
    "    log_rewards = torch.log(env.reward(terminal_states))\n",
    "    return terminal_states, log_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampled_test_set(gfn, env, n=100):\n",
    "    sampler = Sampler(estimator=gfn.pf)\n",
    "    test_trajectories = sampler.sample_trajectories(env=env, n=n)\n",
    "    terminal_states = test_trajectories.last_states\n",
    "    log_rewards = torch.log(env.reward(terminal_states))\n",
    "    return terminal_states, log_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_terminal_states_random, test_log_rewards_random = get_random_test_set(env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_terminal_states_sample, test_log_rewards_sample = get_sampled_test_set(gfn, env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfn.gflownet import GFlowNet\n",
    "\n",
    "@timer\n",
    "def evaluate_GFNEvalS(gfn: GFlowNet, env: HyperGrid, terminal_states, log_rewards):\n",
    "    \"\"\"\n",
    "    Computes the sampling probability of a given terminal state using Backtracking with memoization.\n",
    "\n",
    "    Args:\n",
    "        gfn: An initialized Sampler using the forward policy estimator.\n",
    "        env: The HyperGrid environment instance.\n",
    "        test_trajectories: trajectories used to test the GFlowNet, which contain terminal states and corresponding true rewards\n",
    "\n",
    "    Returns:\n",
    "        spearman_corr_termination: Spearman's Rank Correlation (Modified GFNEvalS, including termination actions)\n",
    "        memo: TensorDict, memo[s] indicates the probability from init_state to s, without counting the probability of termanating at state s.\n",
    "        transition_log_probs: a Tensor list with length of env.n_actions. \n",
    "            transition_log_probs[i][j] indicates the log probability of taking action i at a State env.all_states[j], i in [0, n_actions-1]\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    memo = TensorDict(default_factory=lambda: torch.tensor(['-inf'], requires_grad=False))\n",
    "    transition_log_probs = get_all_transition_log_probs(env, gfn.pf)\n",
    "    log_probs = []\n",
    "    log_probs_termination = []\n",
    "    # Calculate the log probability and log reward for each terminal state\n",
    "    # for traj in test_trajectories:\n",
    "    for terminal_state in tqdm(terminal_states, desc=\"Evaluating test set...\"):\n",
    "        log_prob=compute_log_probability(env, gfn, terminal_state, memo, transition_log_probs)\n",
    "        log_probs.append(log_prob.detach().numpy())\n",
    "        log_prob_termination = compute_log_prob_termination(env, terminal_state, memo, transition_log_probs) \n",
    "        log_probs_termination.append(log_prob_termination.detach().numpy())\n",
    "    # 9 - Compute Spearman's Rank Correlation\n",
    "    spearman_corr_termination, _ = spearmanr(log_probs_termination, log_rewards.detach())\n",
    "    print(f\"Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): {spearman_corr_termination}. Runtime: {time.time()-start_time} seconds.\")\n",
    "    return spearman_corr_termination, memo, transition_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set...: 100%|██████████| 100/100 [00:00<00:00, 114.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.4113598445305147. Runtime: 0.8838462829589844 seconds.\n",
      "Function 'evaluate_GFNEvalS' executed in 0.8842 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spearman_corr_termination, memo, transition_log_probs = evaluate_GFNEvalS(gfn, env, test_terminal_states_random, test_log_rewards_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set...: 100%|██████████| 100/100 [00:00<00:00, 105.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Modified GFNEvalS, including termination actions): 0.870711363609113. Runtime: 0.9630138874053955 seconds.\n",
      "Function 'evaluate_GFNEvalS' executed in 0.9636 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = evaluate_GFNEvalS(gfn, env, test_terminal_states_sample, test_log_rewards_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "@timer\n",
    "def evaluate_GFNEvalS_with_monte_carlo(gfn: GFlowNet, env: HyperGrid, terminal_states, log_rewards, sample_multiples=20):\n",
    "    start_time = time.time()\n",
    "    sampler = Sampler(estimator=gfn.pf)\n",
    "    # Generate a large number of samples as monte carlo experiment to count occurrences of appeared terminal states\n",
    "    n_samples = sample_multiples * env.n_states\n",
    "    occurrences = count_occurrences_with_monte_carlo(env, sampler, n_samples=n_samples)\n",
    "    #\n",
    "    log_probs_monte_carlo = []\n",
    "    for terminal_state in tqdm(terminal_states, desc=\"Evaluating GFNEvalS with monte carlo\"):\n",
    "        log_prob = compute_log_prob_with_monte_carlo(occurrences, terminal_state, n_samples)\n",
    "        log_probs_monte_carlo.append(log_prob.detach().numpy())\n",
    "    # Compute Spearman's Rank Correlation\n",
    "    spearman_corr_monte_carlo, _ = spearmanr(log_probs_monte_carlo, log_rewards)\n",
    "    print(f\"Spearman's Rank Correlation (Monte Carlo): {spearman_corr_monte_carlo}. MC sample number: {n_samples}. Runtime: {time.time()-start_time} seconds\")\n",
    "    return spearman_corr_monte_carlo, occurrences, log_probs_monte_carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terminal_states: 100%|██████████| 81920/81920 [00:00<00:00, 223622.69it/s]\n",
      "Evaluating GFNEvalS with monte carlo: 100%|██████████| 100/100 [00:00<00:00, 35499.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Monte Carlo): 0.41592602767382925. MC sample number: 81920. Runtime: 8.38442587852478 seconds\n",
      "Function 'evaluate_GFNEvalS_with_monte_carlo' executed in 8.3844 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = evaluate_GFNEvalS_with_monte_carlo(gfn, env, test_terminal_states_random, test_log_rewards_random, sample_multiples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terminal_states: 100%|██████████| 81920/81920 [00:00<00:00, 312232.11it/s]\n",
      "Evaluating GFNEvalS with monte carlo: 100%|██████████| 100/100 [00:00<00:00, 38657.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Rank Correlation (Monte Carlo): 0.8707819195578946. MC sample number: 81920. Runtime: 8.04563570022583 seconds\n",
      "Function 'evaluate_GFNEvalS_with_monte_carlo' executed in 8.0457 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spearman_corr_monte_carlo, occurrences, log_probs_monte_carlo = evaluate_GFNEvalS_with_monte_carlo(gfn, env, test_terminal_states_sample, test_log_rewards_sample, sample_multiples=20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
